{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a9d66aa7bfc4236a4e035c70ccdd16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_032e6a19ae8c4dc38ff66dc19b90d084",
              "IPY_MODEL_61b00b41319643d6a8ecd29a8a0870ee",
              "IPY_MODEL_e1866b6a54294fe69e7e6bf49cf80dce"
            ],
            "layout": "IPY_MODEL_2e92bb0e78dc48d2ad8a781c9ee10d8f"
          }
        },
        "032e6a19ae8c4dc38ff66dc19b90d084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c7c388fd38740ce96764dc14dac2245",
            "placeholder": "​",
            "style": "IPY_MODEL_3daa552103114da5b96fe2fd9389733f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "61b00b41319643d6a8ecd29a8a0870ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f27dd21bf0be41d19d463af514ad93a3",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69d0bafc22e04621b2f4fcf969ce4205",
            "value": 3
          }
        },
        "e1866b6a54294fe69e7e6bf49cf80dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b676041019af4c1bb83e82d554109e6c",
            "placeholder": "​",
            "style": "IPY_MODEL_4553a5966b4a44b4903130ffabd82707",
            "value": " 3/3 [00:03&lt;00:00,  1.03it/s]"
          }
        },
        "2e92bb0e78dc48d2ad8a781c9ee10d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c7c388fd38740ce96764dc14dac2245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3daa552103114da5b96fe2fd9389733f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f27dd21bf0be41d19d463af514ad93a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d0bafc22e04621b2f4fcf969ce4205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b676041019af4c1bb83e82d554109e6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4553a5966b4a44b4903130ffabd82707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "access_token = \"YOUR ACCESS TOKEN HERE\""
      ],
      "metadata": {
        "id": "Z5JeH7SoglMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Requirement\n",
        "!pip install git+https://github.com/huggingface/transformers@main\n",
        "!pip install causal-conv1d>=1.2.0\n",
        "!pip install mamba-ssm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwqf_wfp-L_s",
        "outputId": "8ec197ca-cc94-4e56-998c-ac4e76ee8804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers@main\n",
            "  Cloning https://github.com/huggingface/transformers (to revision main) to /tmp/pip-req-build-b5b4to2o\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-b5b4to2o\n",
            "  Resolved https://github.com/huggingface/transformers to commit b32bf85b58260f05da7f3623dca722f9780d2cbc\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0.dev0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0.dev0) (2024.2.2)\n",
            "Requirement already satisfied: mamba-ssm in /usr/local/lib/python3.10/dist-packages (1.2.0.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.2.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (1.11.1.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (0.7.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.40.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm) (12.4.99)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing necessary library"
      ],
      "metadata": {
        "id": "wQHgW-Zohg-M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwjhyUlouJu4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import sys\n",
        "import gc\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "S6wQLmvW7xdF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c7c9cbda-675f-4a2b-ff32-d23ec40f24c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling parameter Benchmark\n"
      ],
      "metadata": {
        "id": "uTPDXIdRy828"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_perplexity(model, tokenizer, dataloader):\n",
        "#   model.eval()\n",
        "#   total_loss = 0\n",
        "#   with torch.no_grad():\n",
        "#     for batch in dataloader:\n",
        "#       input_ids, label = batch[\"input_ids\"].to(device), batch[\"labels\"]\n",
        "#       outputs = model(input_ids, labels=labels)\n",
        "#       total_loss += outputs.loss.item()\n",
        "#   average_loss = total_loss / len(dataloader)\n",
        "#   perplexity = torch.exp(torch.tensor(average_loss))\n",
        "#   return perplexity.item()\n"
      ],
      "metadata": {
        "id": "NzGBi3uksa3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_time_and_memory(model, tokenizer, prompt, max_new_tokens):\n",
        "    # Put on GPU\n",
        "    model.to(device)\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    # Clear any cached memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.synchronize() # Wait for all kernels to complette\n",
        "\n",
        "    start_mem = torch.cuda.memory_allocated(device) # Get starting memory usage\n",
        "    start_peak_mem = torch.cuda.max_memory_allocated(device) # Get peak memory usage\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad(): # Temporarily disable gradient to save memory\n",
        "        outputs = model.generate(input_ids, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    torch.cuda.synchronize() # Makesure that all CUDA operations have completed\n",
        "    end_mem = torch.cuda.memory_allocated(device) # Get ending memory usage\n",
        "    end_peak_mem = torch.cuda.max_memory_allocated(device) # Get ending peak memory usage\n",
        "\n",
        "    time_taken = end_time - start_time\n",
        "    memory_used = end_mem - start_mem\n",
        "    peak_memory_used = end_peak_mem - start_peak_mem\n",
        "    output_length = outputs.shape[1] if outputs is not None else 0\n",
        "\n",
        "    return time_taken, memory_used, peak_memory_used, output_length\n"
      ],
      "metadata": {
        "id": "XtLWjIibt7Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_benchmarks(prompts, max_new_tokens_list, model_info):\n",
        "    results = []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        input_length = len(prompt)\n",
        "        for max_new_tokens in max_new_tokens_list:\n",
        "            for model_name, details in model_info.items():\n",
        "                tokenizer = details[\"tokenizer\"]\n",
        "                model = details[\"model\"].to(device)\n",
        "\n",
        "                time_taken, memory_used, peak_memory_used, output_length = measure_time_and_memory(model, tokenizer, prompt, max_new_tokens)\n",
        "\n",
        "                # deleting the model and clearing cache\n",
        "                del model\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                results.append({\n",
        "                    \"GPU\": \"V100 High RAM\",\n",
        "                    \"Model\": model_name,\n",
        "                    \"Prompt Length\": input_length,\n",
        "                    \"Max New Tokens\": max_new_tokens,\n",
        "                    \"Time Taken (s)\": time_taken,\n",
        "                    \"Memory Used (MiB)\": memory_used / (1024**2),\n",
        "                    \"Peak Memory Used (MiB)\": peak_memory_used / (1024**2),\n",
        "                    \"Output Length\": output_length,\n",
        "                })\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(\"benchmark_results.csv\", index=False)\n",
        "    print(\"Benchmark results saved to benchmark_results.csv\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "gy0wXOp2yqHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_info = {\n",
        "    # \"Mamba 130M\": {\n",
        "    #     \"tokenizer\": AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\"),\n",
        "    #     \"model\": AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
        "    # },\n",
        "    # \"Mamba 370M\": {\n",
        "    #     \"tokenizer\": AutoTokenizer.from_pretrained(\"state-spaces/mamba-370m-hf\"),\n",
        "    #     \"model\": AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-370m-hf\"),\n",
        "    # },\n",
        "    # \"Mamba 1.4B\": {\n",
        "    #     \"tokenizer\": AutoTokenizer.from_pretrained(\"state-spaces/mamba-1.4b-hf\"),\n",
        "    #     \"model\": AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-1.4b-hf\"),\n",
        "    # },\n",
        "    \"Mamba 2.8B\": {\n",
        "        \"tokenizer\": AutoTokenizer.from_pretrained(\"state-spaces/mamba-2.8b-hf\"),\n",
        "        \"model\": AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-2.8b-hf\"),\n",
        "    },\n",
        "    # \"Gemma 2B\": {\n",
        "    #     \"tokenizer\": AutoTokenizer.from_pretrained(\"google/gemma-2b\", token=access_token),\n",
        "    #     \"model\": AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", token=access_token),\n",
        "    # },\n",
        "      # \"Phi 2.7B\": {\n",
        "      #   \"tokenizer\": AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True),\n",
        "      #   \"model\": AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True),\n",
        "      # }\n",
        "    }\n",
        "\n",
        "\n",
        "for info in model_info.values():\n",
        "  info[\"model\"] = info[\"model\"].to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "8a9d66aa7bfc4236a4e035c70ccdd16e",
            "032e6a19ae8c4dc38ff66dc19b90d084",
            "61b00b41319643d6a8ecd29a8a0870ee",
            "e1866b6a54294fe69e7e6bf49cf80dce",
            "2e92bb0e78dc48d2ad8a781c9ee10d8f",
            "6c7c388fd38740ce96764dc14dac2245",
            "3daa552103114da5b96fe2fd9389733f",
            "f27dd21bf0be41d19d463af514ad93a3",
            "69d0bafc22e04621b2f4fcf969ce4205",
            "b676041019af4c1bb83e82d554109e6c",
            "4553a5966b4a44b4903130ffabd82707"
          ]
        },
        "id": "IrJ5jlMhcrdL",
        "outputId": "9515331f-83f9-4289-f46f-45152f0f1365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a9d66aa7bfc4236a4e035c70ccdd16e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Explain the concept of blockchain to a 10-year-old.\",\n",
        "    \"Describe a day in the life of an astronaut on Mars in 2040.\",\n",
        "    \"Imagine a society where human emotions can be transferred as data. How would this technological advancement affect interpersonal relationships, art, and mental health?\",\n",
        "    \"Explain the potential impacts of quantum computing on global cybersecurity infrastructure. Consider both the advancements in cryptography and the potential vulnerabilities introduced\",\n",
        "]\n",
        "\n",
        "max_new_tokens_list = [50, 100, 150, 200]\n",
        "\n",
        "df = run_benchmarks(prompts, max_new_tokens_list, model_info)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zleVkwCan5_6",
        "outputId": "1f43af97-6e4c-41ab-8014-e6e0c31d006c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmark results saved to benchmark_results.csv\n",
            "              GPU       Model  Prompt Length  Max New Tokens  Time Taken (s)  \\\n",
            "0   V100 High RAM  Mamba 2.8B             51              50        6.721991   \n",
            "1   V100 High RAM  Mamba 2.8B             51             100        5.748982   \n",
            "2   V100 High RAM  Mamba 2.8B             51             150        8.839046   \n",
            "3   V100 High RAM  Mamba 2.8B             51             200       11.703989   \n",
            "4   V100 High RAM  Mamba 2.8B             59              50        2.836045   \n",
            "5   V100 High RAM  Mamba 2.8B             59             100        5.807149   \n",
            "6   V100 High RAM  Mamba 2.8B             59             150        8.686378   \n",
            "7   V100 High RAM  Mamba 2.8B             59             200       12.032186   \n",
            "8   V100 High RAM  Mamba 2.8B            167              50        2.783913   \n",
            "9   V100 High RAM  Mamba 2.8B            167             100        5.947925   \n",
            "10  V100 High RAM  Mamba 2.8B            167             150        8.703326   \n",
            "11  V100 High RAM  Mamba 2.8B            167             200       11.368320   \n",
            "12  V100 High RAM  Mamba 2.8B            182              50        2.862670   \n",
            "13  V100 High RAM  Mamba 2.8B            182             100        5.739160   \n",
            "14  V100 High RAM  Mamba 2.8B            182             150        8.636116   \n",
            "15  V100 High RAM  Mamba 2.8B            182             200       11.434747   \n",
            "\n",
            "    Memory Used (MiB)  Peak Memory Used (MiB)  Output Length  \n",
            "0            8.125488               36.751953             64  \n",
            "1            0.000977                0.000000            114  \n",
            "2            0.001465                0.000000            164  \n",
            "3            0.001953                0.000000            214  \n",
            "4            0.000977                0.372559             66  \n",
            "5            0.000977                0.000000            116  \n",
            "6            0.001465                0.000000            166  \n",
            "7            0.001953                0.000000            216  \n",
            "8            0.000977                2.051270             77  \n",
            "9            0.000977                0.000000            127  \n",
            "10           0.001465                0.000000            177  \n",
            "11           0.001953                0.000000            227  \n",
            "12           0.000977                0.000000             77  \n",
            "13           0.000977                0.000000            127  \n",
            "14           0.001465                0.000000            177  \n",
            "15           0.001953                0.000000            227  \n"
          ]
        }
      ]
    }
  ]
}